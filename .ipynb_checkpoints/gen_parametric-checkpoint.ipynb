{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a892a329-86ca-49ca-ac90-66563c73e7b6",
   "metadata": {},
   "source": [
    "# Load image VAE-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "922a33bd-0bf6-4fcc-aaa3-c8d20c47130f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\idgan-master\n"
     ]
    }
   ],
   "source": [
    "%cd \"beta_vae_idgan\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c29edd5-9a24-4a16-899d-ca214d1b7fd8",
   "metadata": {},
   "source": [
    "## Utility_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4924099-dc8d-47f0-a90c-39abda4b9c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from PIL import Image\n",
    "import os.path\n",
    "import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "from math import sqrt\n",
    "\n",
    "import pandas as pd\n",
    "from vrProjector_master import vrProjector\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6afa7d50-dbc6-4d14-8cc4-9af6d846b1cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from os import path\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from gan_training import utils\n",
    "from gan_training.inputs import get_dataset\n",
    "from gan_training.distributions import get_ydist, get_zdist\n",
    "from gan_training.eval import Evaluator\n",
    "from gan_training.config import (\n",
    "    load_config, build_models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af408562-cadf-42f5-9e5d-41f4cd192c3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_one_hot(label, N):\n",
    "    size = list(label.size())\n",
    "    label = label.view(-1).cpu()   #reshape to a long vector\n",
    "    ones = torch.sparse.torch.eye(N)\n",
    "    ones = ones.index_select(0, label)   #turn to one hot\n",
    "    size.append(N)  #reshape to h*w*channel classes\n",
    "    return ones.view(*size).squeeze(1).permute(0,3,1,2)\n",
    "\n",
    "def discretize_to_order_labels(t):\n",
    "    tensor=t.cpu()\n",
    "    bins = torch.tensor([-0.125,0.125, 0.375, 0.625, 0.875,1.125])\n",
    "    inds = torch.bucketize(tensor, bins)\n",
    "    tensor_discret = inds.add(-1)\n",
    "    \n",
    "    return tensor_discret\n",
    "\n",
    "def generator_postprocess(x):\n",
    "    n_channels = x.size(1)\n",
    "    \n",
    "    if n_channels ==5:\n",
    "        x_im = x.add(1).div(2).clamp(0,1).round().argmax(dim=1).unsqueeze(1).cpu()/(n_channels-1)\n",
    "        return x_im\n",
    "        \n",
    "    else:\n",
    "        x_shift = x.add(1).div(2).cpu()\n",
    "        x_discret = discretize_to_order_labels(x_shift)\n",
    "\n",
    "        return x_discret.div(4)\n",
    "\n",
    "def decoder_postprocess(x):\n",
    "    n_channels = x.size(1)\n",
    "    \n",
    "    if n_channels ==5:\n",
    "        x_recover = x.softmax(dim=1).argmax(dim=1).unsqueeze(1).cpu()/(n_channels-1)\n",
    "        return x_recover\n",
    "        \n",
    "    else:\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "def show_im(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    \n",
    "def gray_label_to_torch_tensor(path):\n",
    "    example =  cv2.imread(path)\n",
    "    \n",
    "    gt = np.round(np.mean(example,axis=2)/64)\n",
    "    gt = torch.LongTensor(gt)\n",
    "\n",
    "    def get_one_hot(label, N):\n",
    "        size = list(label.size())\n",
    "        label = label.view(-1)   #reshape to a long vector\n",
    "        ones = torch.sparse.torch.eye(N)\n",
    "        ones = ones.index_select(0, label)   #turn to one hot\n",
    "        size.append(N)  #reshape to h*w*channel classes\n",
    "        return ones.view(*size).cuda()\n",
    "\n",
    "\n",
    "    gt_one_hot = get_one_hot(gt, 5)\n",
    "    #print(gt_one_hot)\n",
    "    #print(gt_one_hot.shape)\n",
    "    #print(gt_one_hot.argmax(-1) == gt)  # check if one hot converting correct or not (1:correct)\n",
    "\n",
    "    #gt_remove_edge = gt_one_hot[:,:,1:].permute(2,0,1)\n",
    "    gt_reserve_edge = gt_one_hot.permute(2,0,1)\n",
    "    img_tensor = gt_reserve_edge\n",
    "    \n",
    "    return img_tensor\n",
    "\n",
    "def load_simple_im(path):\n",
    "    example =  cv2.imread(path)\n",
    "    \n",
    "    gt = np.mean(example,axis=2)/256\n",
    "    \n",
    "    gt = (gt-0.5)*2\n",
    "\n",
    "    img_tensor = torch.from_numpy(gt).unsqueeze(0).unsqueeze(0).float()\n",
    "    \n",
    "    return img_tensor\n",
    "\n",
    "class solargan_im_trainset(Dataset):\n",
    "    def __init__(self,images, loader):\n",
    "        \n",
    "        self.images = images #image path\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn = self.images[index]\n",
    "        tensor = self.loader(fn)\n",
    "        return tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "class CheckpointIO(object):\n",
    "    def __init__(self, checkpoint_dir='./chkpts', **kwargs):\n",
    "        self.module_dict = kwargs\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "    def register_modules(self, **kwargs):\n",
    "        self.module_dict.update(kwargs)\n",
    "\n",
    "    def save(self, it, filename):\n",
    "        filename = os.path.join(self.checkpoint_dir, filename)\n",
    "\n",
    "        outdict = {'it': it}\n",
    "        for k, v in self.module_dict.items():\n",
    "            outdict[k] = v.state_dict()\n",
    "        torch.save(outdict, filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        filename = os.path.join(self.checkpoint_dir, filename)\n",
    "\n",
    "        if os.path.exists(filename):\n",
    "            tqdm.write('=> Loading checkpoint...')\n",
    "            out_dict = torch.load(filename,map_location=torch.device('cpu')) ##only use CPU here!!!\n",
    "            it = out_dict['it']\n",
    "            for k, v in self.module_dict.items():\n",
    "                if k in out_dict:\n",
    "                    v.load_state_dict(out_dict[k])\n",
    "                else:\n",
    "                    tqdm.write('Warning: Could not find %s in checkpoint!' % k)\n",
    "        else:\n",
    "            it = -1\n",
    "\n",
    "        return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74826b09-61e6-4b4d-8a00-19e794679c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_imsample_plot(tensor,title,col_space = 50,nrow=6, padding=2, pad_value=0):\n",
    "    cmap = {\n",
    "    '0'         : \"#FFFFFF\",      ## \tblack edge: black\n",
    "    '1'         : \"#DAA520\",      ## \tground: Khaki    \n",
    "    '2'         : \"#CD5C5C\",      ## \topaque surfaces: FireBrick\n",
    "    '3'         : \"#87CEFA\",      ## \tglazing: MediumTurquoise\n",
    "    '4'         : \"#F0FFFF\",      ## \tsky：Azure \n",
    "}   \n",
    "\n",
    "    tensor_grid = make_grid(tensor, nrow=nrow, padding=padding, pad_value=pad_value)\n",
    "    np_img = tensor_grid.numpy()\n",
    "                          \n",
    "    p = figure(title=title,tooltips=[(\"x\", \"$x\"), (\"y\", \"$y\"), (\"value\", \"@image\")],x_range = ([0,np_img.shape[1]+col_space]))\n",
    "\n",
    "\n",
    "    mapper = CategoricalColorMapper(palette=list(cmap.values()), factors=list(cmap.keys()))\n",
    "    \n",
    "                              \n",
    "    p.image(image = [np.flip((np_img[1,:,:]*4).astype(int).astype(str),axis=0)], x=0, y=0, dw=np_img.shape[1], dh=np_img.shape[2],color_mapper=mapper)\n",
    "    \n",
    "\n",
    "    \n",
    "    p.axis.visible = False\n",
    "    p.grid.visible = False\n",
    "    p.outline_line_alpha=0\n",
    "    p.outline_line_width = 3\n",
    "    \n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "845a0398-2b9c-46d5-8470-d505e9757511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize_one_image(gray_image, n):\n",
    "    \n",
    "    def colormap(n):\n",
    "        cmap=np.zeros([n, 3]).astype(np.uint8)\n",
    "        cmap[0] = np.array([255,255,255])  ## \tblack edge: blac\n",
    "        cmap[1] = np.array([218,165,32])   ## \tground: Goldenrod\n",
    "        cmap[2] = np.array([205,92,92])    ## \topaque surfaces: IndianRed\n",
    "        cmap[3] = np.array([135,206,250])  ## \tglazing: LightSkyBlue\n",
    "        cmap[4] = np.array([240,255,255\t]) ## \tsky: Azure\n",
    "\n",
    "        return cmap\n",
    "    \n",
    "    cmap = colormap(n)\n",
    "    size = gray_image.size()  # 网络output的大小\n",
    "    color_image = torch.ByteTensor(3, size[1], size[2]).fill_(0) \n",
    "\n",
    "    for label in range(0, len(cmap)):# 依次遍历label的颜色表\n",
    "        mask = gray_image[0] == label  \n",
    "        #gray_image[0] 是将三维的图像，以【1, 10, 10】为例，变成二维【10,10】,这个参数是外部传入，这里确保是二维单通道就行了\n",
    "        #gray_image[0] == label 意思是将 gray_image[0]中为label值的元素视为true或者1，其他的元素为False 或0，得到mask的布尔图\n",
    "\n",
    "        color_image[0][mask] = cmap[label][0] #取取颜色表中为label列表(【a,b,c】)的a\n",
    "        #color_image[0]是取三通道模板中的单通道 ，然后把mask放上去\n",
    "        color_image[1][mask] = cmap[label][1]  # 取b\n",
    "        color_image[2][mask] = cmap[label][2]#   取c\n",
    "\n",
    "    return color_image\n",
    "\n",
    "def gen_pure_white(w,h):\n",
    "\n",
    "        b = g =r  = np.ones((w,h), dtype=np.uint8)*255\n",
    "\n",
    "\n",
    "        white = cv2.merge([b, g, r])\n",
    "        \n",
    "        return white\n",
    "\n",
    "def cubemap_back_to_fisheye(single_im_tensor,n_channels=5, outsize=256, padding=4):\n",
    "    x_1 = colorize_one_image(single_im_tensor,5)\n",
    "\n",
    "    npimg = np.transpose(x_1.numpy(), (1,2,0))\n",
    "    npimg_front = npimg[32:96,32:96,:]\n",
    "    npimg_top2rotate=np.vstack([gen_pure_white(32,64), npimg[0:32,32:96,:]])\n",
    "    npimg_top = np.rot90(npimg_top2rotate,1)\n",
    "    npimg_bottom2rotate=np.vstack([npimg[96:128,32:96,:],gen_pure_white(32,64)])\n",
    "    npimg_bottom = np.rot90(npimg_bottom2rotate,-1)\n",
    "    npimg_right=np.hstack([npimg[32:96,96:128,:],gen_pure_white(64,32)])\n",
    "    npimg_left=np.hstack([gen_pure_white(64,32),npimg[32:96,0:32,:]])\n",
    "    npimg_back=gen_pure_white(64,64)\n",
    "    \n",
    "    source = vrProjector.CubemapProjection()\n",
    "    #source.loadImages(\"cubemap_reverse/left.png\",\"cubemap_reverse/front.png\",\"cubemap_reverse/right.png\",\"cubemap_reverse/back.png\",\"cubemap_reverse/top.png\",\"cubemap_reverse/bottom.png\")\n",
    "    source.readImageArrays(npimg_left,npimg_front,npimg_right,npimg_back,npimg_top,npimg_bottom)\n",
    "    #source.set_use_bilinear(True)\n",
    "\n",
    "\n",
    "    out = vrProjector.SideBySideFisheyeProjection()\n",
    "    out.initImage((outsize-padding*2)*2,outsize-padding*2)\n",
    "    out.reprojectToThis(source)\n",
    "    #out.saveImage(\"cubemap_reverse/fisheye.png\")\n",
    "    x_fish_dual = out.outputImage(\"fisheye\")\n",
    "    \n",
    "    x_fish = x_fish_dual[:,outsize-padding*2:(outsize-padding*2)*2,:]\n",
    "    blk_mask=x_fish==0\n",
    "    x_fish[blk_mask] = 255\n",
    "    x_fish = np.pad(x_fish, ((padding,padding), (padding,padding),(0, 0)), 'constant', constant_values=255)\n",
    "\n",
    "\n",
    "    #plt.imshow(x_fish)\n",
    "    #plt.axis(\"off\")\n",
    "    \n",
    "    return x_fish\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38cac018-1b1e-43dd-95c7-80d4b0e0a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fisheye_grid(tensor, mode,batch_size=36,n_rows = 3,im_size=256,padding=4):\n",
    "    if mode == 'gt':\n",
    "        factor = 1\n",
    "    else:\n",
    "        factor = 4\n",
    "    \n",
    "    np_allim = np.empty(shape=(0,im_size,im_size,4))\n",
    "    for ix in range(batch_size):\n",
    "        \n",
    "        tensor_ix = tensor[ix]*factor\n",
    "        npimg_ix = cubemap_back_to_fisheye(tensor_ix, n_channels = 5,outsize=im_size,padding=padding)/255\n",
    "        np_allim = np.concatenate((np_allim,np.expand_dims(npimg_ix[:,:,:],0)),axis=0)\n",
    "        \n",
    "    tensor_allim = torch.from_numpy(np.transpose(np_allim,(0,3,1,2)))\n",
    "    tensor_allim_grid = make_grid(tensor_allim, nrow=n_rows, padding=0, pad_value=0)\n",
    "    np_allim_grid = np.transpose(tensor_allim_grid.numpy(),(1,2,0))\n",
    "        \n",
    "    #print(np.unique(tensor_allim_grid.numpy()))\n",
    "    #plt.figure(figsize=(10,10))\n",
    "    #plt.imshow(np_allim_grid)\n",
    "    #plt.axis(\"off\")\n",
    "        \n",
    "    return np_allim_grid*255\n",
    "\n",
    "def make_grid_fisheye_with_plt(file_name,tensor1,tensor2,tensor3,nrows1=3,nrows2=3,nrows3=3,batch_size1=21,batch_size2=21,batch_size3=21):\n",
    "    \n",
    "    npimg1 = make_fisheye_grid(tensor1, 'gt', batch_size=batch_size1,n_rows=nrows1, im_size=256,padding=4)/255\n",
    "    npimg2 = make_fisheye_grid(tensor2, 'res', batch_size=batch_size2,n_rows=nrows2,im_size=256,padding=4)/255\n",
    "    npimg3 = make_fisheye_grid(tensor3, 'vae', batch_size=batch_size3,n_rows=nrows3,im_size=256,padding=4)/255\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10),dpi=1200)\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                     nrows_ncols=(1, 3),  # creates 2x2 grid of axes\n",
    "                     axes_pad=0.4,  # pad between axes in inch.\n",
    "                     )\n",
    "\n",
    "    for ax, im in zip(grid, [npimg1, npimg2, npimg3]):\n",
    "        # Iterating over the grid returns the Axes.\n",
    "        ax.imshow(im)\n",
    "        ax.axis('off')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(file_name, format=\"svg\")\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58442c36-0f5d-4fd9-86ef-64817db3e1ca",
   "metadata": {},
   "source": [
    "## Configs_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26474f38-d107-480b-a71c-65a6a3611bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configres_path = 'graycube_imres_test.yaml'\n",
    "configres_path = 'sbe_imtest_single.yaml'\n",
    "\n",
    "#configs\n",
    "configres = load_config(configres_path)\n",
    "\n",
    "c_dim = configres['dvae']['c_dim']\n",
    "out_res_name = configres['test']['out_name']\n",
    "batch_size = configres['test']['batch_size']\n",
    "checkpoint_res_dir = path.join(out_res_name, 'chkpts')\n",
    "\n",
    "\n",
    "#c_dim_wwr = 30\n",
    "#lim = 3\n",
    "#lim_d = -3\n",
    "#ncol = 7\n",
    "\n",
    "c_meaningful = [12,16,29] ##12:z-direction 16: y-direction, 29:orbit_clockwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d91ebe-2191-4dc9-9be3-c53df4f7e398",
   "metadata": {},
   "source": [
    "## Load_trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b76f1199-79ec-4c26-ab04-9589f25d7795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load_batch_samples\n",
    "test_dataset = get_dataset(\n",
    "    name=configres['data']['type'],\n",
    "    data_dir=configres['data']['test_dir'],\n",
    "    size=configres['data']['img_size'],\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=False, pin_memory=True, sampler=None, drop_last=True\n",
    ")\n",
    "\n",
    "ntest = batch_size\n",
    "ntest = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb7eff15-55e7-4806-a5fc-c71ee255f30e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint...\n"
     ]
    }
   ],
   "source": [
    "# Logger\n",
    "checkpoint_res_io = CheckpointIO(\n",
    "    checkpoint_dir=checkpoint_res_dir\n",
    ")\n",
    "\n",
    "# Create models\n",
    "dvae, generator_res, discriminator_res = build_models(configres)\n",
    "dvae_ckpt_path = os.path.join('outputs', configres['dvae']['runname'], 'chkpts', configres['dvae']['ckptname'])\n",
    "dvae_ckpt = torch.load(dvae_ckpt_path,map_location=torch.device('cpu'))['model_states']['net']\n",
    "dvae.load_state_dict(dvae_ckpt)\n",
    "\n",
    "# Put models on gpu if needed\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if is_cuda else \"cpu\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "dvae = dvae.to(device)\n",
    "generator_res = generator_res.to(device)\n",
    "discriminator_res = discriminator_res.to(device)\n",
    "\n",
    "\n",
    "# Use multiple GPUs if possible\n",
    "generator_res = nn.DataParallel(generator_res)\n",
    "discriminator_res = nn.DataParallel(discriminator_res)\n",
    "\n",
    "\n",
    "# Register modules to checkpoint\n",
    "checkpoint_res_io.register_modules(\n",
    "    generator=generator_res,\n",
    "    discriminator=discriminator_res,\n",
    ")\n",
    "\n",
    "\n",
    "# Test generator\n",
    "generator_res_test = generator_res\n",
    "\n",
    "# Distributions\n",
    "zdist = get_zdist(configres['z_dist']['type'], configres['z_dist']['dim'],\n",
    "                  device=device)\n",
    "\n",
    "# Load checkpoint if existant\n",
    "it_res = checkpoint_res_io.load('model_00499999.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cac13fc-89c4-4788-8dae-1516e2f2a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wwr_traversing(x_real_test,c_dim_wwr = 30, lim = 1, lim_d=-2.6, ncol=4):\n",
    "\n",
    "    x_real_test_batch_small = x_real_test[:,:,:]\n",
    "\n",
    "    x_real_shift = x_real_test_batch_small.add(1).div(2)\n",
    "\n",
    "    if x_real_shift.size(1) == 1:\n",
    "        x_real_disc = discretize_to_order_labels(x_real_shift)\n",
    "        x_real_onehot = get_one_hot(x_real_disc, 5)\n",
    "        x_real_onehot = x_real_onehot.to(device)\n",
    "\n",
    "        c, c_mu, c_logvar = cs = dvae(x_real_onehot, encode_only=True)\n",
    "\n",
    "    else:\n",
    "        x_real_shift = x_real_shift.to(device)\n",
    "        c, c_mu, c_logvar = cs = dvae(x_real_shift, encode_only=True)\n",
    "\n",
    "    interpolation = torch.linspace(lim_d, lim, ncol)\n",
    "\n",
    "    idganres_samples_p = []\n",
    "    dvae_samples_p = []\n",
    "\n",
    "    z = zdist.sample((batch_size,))\n",
    "\n",
    "    for i in range(x_real_shift.size(0)):\n",
    "\n",
    "        c_ = c_mu[i:i+1]\n",
    "        z_ = z[i:i+1]\n",
    "        c_zero = torch.zeros_like(c_)\n",
    "\n",
    "        for val in interpolation:\n",
    "            c_p = c_\n",
    "            c_p[:, c_dim_wwr] = val\n",
    "\n",
    "            #c_zero[:, c_dim_wwr] = val\n",
    "            #c_p = c_ + c_zero\n",
    "            z_p_ = torch.cat([z_, c_p], 1)\n",
    "\n",
    "            idganres_sample_p = generator_postprocess(generator_res(z_p_)).data.cpu()\n",
    "            idganres_samples_p.append(idganres_sample_p)\n",
    "\n",
    "            dvae_sample_p = decoder_postprocess(dvae(c=c_p, decode_only=True)).data.cpu()\n",
    "            dvae_samples_p.append(dvae_sample_p)\n",
    "\n",
    "\n",
    "    idganres_samples_p = torch.cat(idganres_samples_p, dim=0)\n",
    "\n",
    "    dvae_samples_p = torch.cat(dvae_samples_p, dim=0)\n",
    "\n",
    "    return x_real_disc/4, idganres_samples_p,dvae_samples_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b441166-a3e1-492e-bb22-af7780349c90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wwr_traversing_gif(folder, x_real_test, c_dim_wwr = 30, lim = 1, lim_d=-2.6, ncol=12,fps=3):\n",
    "    \n",
    "    target_folder = folder+ '/img/'\n",
    "\n",
    "    x_real_test_batch_small = x_real_test[:,:,:]\n",
    "\n",
    "    x_real_shift = x_real_test_batch_small.add(1).div(2)\n",
    "\n",
    "    if x_real_shift.size(1) == 1:\n",
    "        x_real_disc = discretize_to_order_labels(x_real_shift)\n",
    "        x_real_onehot = get_one_hot(x_real_disc, 5)\n",
    "        x_real_onehot = x_real_onehot.to(device)\n",
    "\n",
    "        c, c_mu, c_logvar = cs = dvae(x_real_onehot, encode_only=True)\n",
    "\n",
    "    else:\n",
    "        x_real_shift = x_real_shift.to(device)\n",
    "        c, c_mu, c_logvar = cs = dvae(x_real_shift, encode_only=True)\n",
    "\n",
    "    interpolation = torch.linspace(lim_d, lim, ncol)\n",
    "\n",
    "\n",
    "    z = zdist.sample((batch_size,))\n",
    "    ix = 0\n",
    "\n",
    "    for i in range(x_real_shift.size(0)):\n",
    "\n",
    "        c_ = c_mu[i:i+1]\n",
    "        z_ = z[i:i+1]\n",
    "        c_zero = torch.zeros_like(c_)\n",
    "\n",
    "        for val in interpolation:\n",
    "            c_p = c_\n",
    "            c_p[:, c_dim_wwr] = val\n",
    "\n",
    "            #c_zero[:, c_dim_wwr] = val\n",
    "            #c_p = c_ + c_zero\n",
    "            z_p_ = torch.cat([z_, c_p], 1)\n",
    "\n",
    "            idganres_sample_p = generator_postprocess(generator_res(z_p_)).data.cpu().squeeze(0)\n",
    "            #print(idganres_sample_p.size())\n",
    "            x_gan = Image.fromarray(cubemap_back_to_fisheye(idganres_sample_p*4, n_channels = 5))\n",
    "            gan_file = target_folder+str(ix)+'.PNG'\n",
    "            x_gan.save(gan_file)\n",
    "            \n",
    "            ix+=1\n",
    "            \n",
    "    img_names_gan = [target_folder+str(i)+'.PNG' for i in range(ncol)]\n",
    "    #img_names.reverse()\n",
    "    clip_gan = ImageSequenceClip(img_names_gan,fps=fps)\n",
    "\n",
    "    gan_gif_file = folder+'/'+str(folder)+'_img.gif'\n",
    "\n",
    "    clip_gan.write_gif(gan_gif_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c6e0f7-465e-4843-a9d8-246c5ba0bece",
   "metadata": {},
   "source": [
    "## WWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f2db68c-7d1a-43aa-b815-d0d338c67a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 128, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_real_test_batch = utils.get_nsamples(test_loader, ntest)\n",
    "x_real_test_batch=load_simple_im('zh_graycube/zh_graycube__91.PNG')\n",
    "x_real_test_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba6f1208-deaa-4122-a0a2-5b44854cfe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cf1c439-4495-4838-82d8-5b7ce651202c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\att_feat_processing\\gif\n"
     ]
    }
   ],
   "source": [
    "cd att_feat_processing/gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0075f21d-7fdc-493e-9f48-82d27b3ed1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file wwr_30/wwr_30_img.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    }
   ],
   "source": [
    "##12:z-direction 16: y-direction, 29:orbit_clockwise\n",
    "dim_list = [12,16,30]\n",
    "dim_ix = 2\n",
    "ncol=3\n",
    "lim_d = 0.64\n",
    "lim = -1.5\n",
    "trav_dim = dim_list[dim_ix]\n",
    "wwr_traversing_gif('wwr_30',x_real_test_batch, c_dim_wwr = trav_dim, lim_d = lim_d, lim=lim, ncol=12,fps=3)\n",
    "\n",
    "#make_grid_fisheye_with_plt('sbe_im_91.svg',x_gt*4,x_gan,x_vae,nrows1=1,nrows2=ncol,nrows3=ncol,batch_size1=1,batch_size2=ncol*1,batch_size3=ncol*1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

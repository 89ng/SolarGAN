{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a63214-4ebc-42a0-93a4-3cbd306d8588",
   "metadata": {},
   "source": [
    "# Result_time_series_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1360fe8-fe1b-4656-a110-1596b8b64306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\DoppelGANger\n"
     ]
    }
   ],
   "source": [
    "%cd \"DoppelGANger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad4e1761-c0a2-4359-b01c-fcb6b1cfbd8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\zhang\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\zhang\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\zhang\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\zhang\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\zhang\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\zhang\\DoppelGANger\\gan\\util.py:4: UserWarning: matplotlib.pyplot as already been imported, this call will have no effect.\n",
      "  matplotlib.use(\"Agg\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gan import output\n",
    "sys.modules[\"output\"] = output\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from gan.doppelganger import DoppelGANger\n",
    "from gan.load_data import load_data\n",
    "from gan.network import DoppelGANgerGenerator, Discriminator, AttrDiscriminator\n",
    "from gan.output import Output, OutputType, Normalization\n",
    "import tensorflow as tf\n",
    "from gan.network import DoppelGANgerGenerator, Discriminator, \\\n",
    "    RNNInitialStateType, AttrDiscriminator\n",
    "from gan.util import add_gen_flag, normalize_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47b6c552-37ed-46f1-a97c-fe9248c51e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ab448c-1400-4bd2-a7f3-1be26a199181",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7fc8d6e-5318-4d09-aca0-ac60861b1e6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers of DoppelGANgerGenerator\n",
      "[<tf.Variable 'DoppelGANgerGenerator/attribute_addi/layer0/linear/dense/kernel:0' shape=(52, 100) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/attribute_addi/layer0/linear/dense/bias:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/attribute_addi/layer0/batch_norm/beta:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/attribute_addi/layer0/batch_norm/gamma:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/attribute_addi/layer0/batch_norm/moving_mean:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/attribute_addi/layer0/batch_norm/moving_variance:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/attribute_addi/layer1/linear/dense/kernel:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/attribute_addi/layer1/linear/dense/bias:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/attribute_addi/layer1/batch_norm/beta:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/attribute_addi/layer1/batch_norm/gamma:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/attribute_addi/layer1/batch_norm/moving_mean:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/attribute_addi/layer1/batch_norm/moving_variance:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/attribute_addi/layer2/output0/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/attribute_addi/layer2/output0/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/attribute_addi/layer2/output1/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/attribute_addi/layer2/output1/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/multi_rnn_cell/cell_0/lstm_cell/kernel:0' shape=(210, 400) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/multi_rnn_cell/cell_0/lstm_cell/bias:0' shape=(400,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/multi_rnn_cell/cell_1/lstm_cell/kernel:0' shape=(200, 400) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/multi_rnn_cell/cell_1/lstm_cell/bias:0' shape=(400,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output0/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output0/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output1/linear/dense/kernel:0' shape=(100, 2) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output1/linear/dense/bias:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output2/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output2/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output3/linear/dense/kernel:0' shape=(100, 2) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output3/linear/dense/bias:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output4/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output4/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output5/linear/dense/kernel:0' shape=(100, 2) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output5/linear/dense/bias:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output6/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output6/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output7/linear/dense/kernel:0' shape=(100, 2) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output7/linear/dense/bias:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output8/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output8/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output9/linear/dense/kernel:0' shape=(100, 2) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output9/linear/dense/bias:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output10/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output10/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output11/linear/dense/kernel:0' shape=(100, 2) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output11/linear/dense/bias:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output12/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output12/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output13/linear/dense/kernel:0' shape=(100, 2) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output13/linear/dense/bias:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output14/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output14/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output15/linear/dense/kernel:0' shape=(100, 2) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output15/linear/dense/bias:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output16/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output16/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output17/linear/dense/kernel:0' shape=(100, 2) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output17/linear/dense/bias:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output18/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output18/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output19/linear/dense/kernel:0' shape=(100, 2) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output19/linear/dense/bias:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output20/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output20/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output21/linear/dense/kernel:0' shape=(100, 2) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output21/linear/dense/bias:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output22/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output22/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output23/linear/dense/kernel:0' shape=(100, 2) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output23/linear/dense/bias:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output24/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output24/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output25/linear/dense/kernel:0' shape=(100, 2) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output25/linear/dense/bias:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output26/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output26/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output27/linear/dense/kernel:0' shape=(100, 2) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output27/linear/dense/bias:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output28/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output28/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output29/linear/dense/kernel:0' shape=(100, 2) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output29/linear/dense/bias:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output30/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output30/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output31/linear/dense/kernel:0' shape=(100, 2) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output31/linear/dense/bias:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output32/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output32/linear/dense/bias:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output33/linear/dense/kernel:0' shape=(100, 2) dtype=float32_ref>, <tf.Variable 'DoppelGANgerGenerator/feature/output33/linear/dense/bias:0' shape=(2,) dtype=float32_ref>]\n",
      "Layers of discriminator\n",
      "[<tf.Variable 'discriminator/layer0/linear/dense/kernel:0' shape=(406, 100) dtype=float32_ref>, <tf.Variable 'discriminator/layer0/linear/dense/bias:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'discriminator/layer1/linear/dense/kernel:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'discriminator/layer1/linear/dense/bias:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'discriminator/layer2/linear/dense/kernel:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'discriminator/layer2/linear/dense/bias:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'discriminator/layer3/linear/dense/kernel:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'discriminator/layer3/linear/dense/bias:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'discriminator/layer4/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'discriminator/layer4/linear/dense/bias:0' shape=(1,) dtype=float32_ref>]\n",
      "Layers of attrDiscriminator\n",
      "[<tf.Variable 'attrDiscriminator/layer0/linear/dense/kernel:0' shape=(49, 100) dtype=float32_ref>, <tf.Variable 'attrDiscriminator/layer0/linear/dense/bias:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'attrDiscriminator/layer1/linear/dense/kernel:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'attrDiscriminator/layer1/linear/dense/bias:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'attrDiscriminator/layer2/linear/dense/kernel:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'attrDiscriminator/layer2/linear/dense/bias:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'attrDiscriminator/layer3/linear/dense/kernel:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'attrDiscriminator/layer3/linear/dense/bias:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'attrDiscriminator/layer4/linear/dense/kernel:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'attrDiscriminator/layer4/linear/dense/bias:0' shape=(1,) dtype=float32_ref>]\n",
      "INFO:tensorflow:Restoring parameters from ./results/checkpoint\\model-1357199\n"
     ]
    }
   ],
   "source": [
    "def normalize_attribute(data_att, data_att_outputs, data_att_min, data_att_max):\n",
    "    \n",
    "    data_att_norm = data_att\n",
    "    total_dim = 0\n",
    "    for output in data_att_outputs:\n",
    "        if output.type_ == OutputType.CONTINUOUS:\n",
    "            for _ in range(output.dim):\n",
    "                data_att_norm[:, total_dim] = (data_att_norm[:, total_dim] - data_att_min[total_dim]) / (data_att_max[total_dim] - data_att_min[total_dim])\n",
    "                if output.normalization == Normalization.MINUSONE_ONE:\n",
    "                    data_att_norm[:, total_dim] = data_att_norm[:, total_dim] * 2.0 - 1.0\n",
    "\n",
    "                total_dim += 1\n",
    "        else:\n",
    "            total_dim += output.dim\n",
    "\n",
    "\n",
    "    return data_att_norm\n",
    "\n",
    "attributes = np.load('europe_att_test.npy')\n",
    "#attributes = np.load('seasia_att_test.npy')\n",
    "train_sample_size = attributes.shape[0]\n",
    "\n",
    "features = np.load('europe_feat_test.npy')\n",
    "#features = np.load('seasia_feat_test.npy')\n",
    "\n",
    "features = features.reshape(-1,119,1)\n",
    "\n",
    "features_gt = features\n",
    "\n",
    "gen_flags = np.ones((train_sample_size,119))\n",
    "\n",
    "\n",
    "data_feature_outputs = [\n",
    "\toutput.Output(type_=OutputType.CONTINUOUS,dim=1,normalization=Normalization.ZERO_ONE,is_gen_flag=False)]\n",
    "  #hourly solar radiation\n",
    "\n",
    "data_attribute_outputs = [\n",
    "\toutput.Output(type_=OutputType.CONTINUOUS,dim=1,normalization=Normalization.MINUSONE_ONE,is_gen_flag=False),\n",
    "  #lat\n",
    "\toutput.Output(type_=OutputType.CONTINUOUS,dim=1,normalization=Normalization.MINUSONE_ONE,is_gen_flag=False),\n",
    "  #longi\n",
    "  output.Output(type_=OutputType.CONTINUOUS,dim=1,normalization=Normalization.ZERO_ONE,is_gen_flag=False),\n",
    "  #height\n",
    "  output.Output(type_=OutputType.CONTINUOUS,dim=2,normalization=Normalization.MINUSONE_ONE,is_gen_flag=False),\n",
    "  #norm vector (xy)\n",
    "  #output.Output(type_=OutputType.CONTINUOUS,dim=1,normalization=Normalization.ZERO_ONE,is_gen_flag=False),\n",
    "  #glzr\n",
    "  output.Output(type_=OutputType.CONTINUOUS,dim=32,normalization=Normalization.MINUSONE_ONE,is_gen_flag=False),\n",
    "  #latent_im\n",
    "  output.Output(type_=OutputType.CONTINUOUS,dim=1,normalization=Normalization.ZERO_ONE,is_gen_flag=False),\n",
    "  #mon\n",
    "  output.Output(type_=OutputType.CONTINUOUS,dim=1,normalization=Normalization.MINUSONE_ONE,is_gen_flag=False),\n",
    "  #inc\n",
    "  output.Output(type_=OutputType.CONTINUOUS,dim=8,normalization=Normalization.ZERO_ONE,is_gen_flag=False)]\n",
    "  #weather_stat\n",
    "  #output.Output(type_=OutputType.DISCRETE,dim=656,normalization=None,is_gen_flag=False)]\n",
    "  #shadow mask\n",
    "\n",
    "\n",
    "#necessary inputs\n",
    "data_all = features\n",
    "data_attribut = attributes\n",
    "data_gen_flag = gen_flags\n",
    "\n",
    "sample_len = 17\n",
    "\n",
    "# normalise data\n",
    "(data_feature, data_attribute, data_attribute_outputs,\n",
    " real_attribute_mask) = normalize_per_sample(\n",
    "        data_all, data_attribut, data_feature_outputs,\n",
    "        data_attribute_outputs)\n",
    "\n",
    "# add generation flag to features\n",
    "data_feature, data_feature_outputs = add_gen_flag(\n",
    "    data_feature, data_gen_flag, data_feature_outputs, sample_len)\n",
    "\n",
    "\n",
    "data_attribute_min = np.amin(data_attribute, axis=0)\n",
    "data_attribute_max = np.amax(data_attribute, axis=0)\n",
    "np.save('europe_att_test_min.npy',data_attribute_min)\n",
    "#np.save('seasia_att_test_min.npy',data_attribute_min)\n",
    "np.save('europe_att_test_max.npy',data_attribute_max)\n",
    "#np.save('seasia_att_test_max.npy',data_attribute_max)\n",
    "\n",
    "\n",
    "data_attribute_normlized = normalize_attribute(data_attribute, data_attribute_outputs, data_attribute_min, data_attribute_max)\n",
    "\n",
    "\n",
    "generator = DoppelGANgerGenerator(\n",
    "    feed_back=True,\n",
    "    noise=True,\n",
    "    feature_outputs=data_feature_outputs,\n",
    "    attribute_outputs=data_attribute_outputs,\n",
    "    real_attribute_mask=real_attribute_mask,\n",
    "    attribute_num_units =100,\n",
    "    sample_len=sample_len,\n",
    "    feature_num_units=100,\n",
    "    feature_num_layers=2)\n",
    "\n",
    "discriminator = Discriminator(num_units=100)\n",
    "attr_discriminator = AttrDiscriminator(num_units=100)\n",
    "\n",
    "checkpoint_dir = \"solargan_training/results/checkpoint\"\n",
    "sample_dir = \"solargan_training/results/sample\"\n",
    "time_path = \"solargan_training/results/time/time.txt\"\n",
    "epoch = 200\n",
    "batch_size = 100\n",
    "g_lr = 0.0001\n",
    "d_lr = 0.0001 \n",
    "vis_freq = 1000\n",
    "vis_num_sample = 1\n",
    "d_rounds = 3\n",
    "g_rounds = 1\n",
    "d_gp_coe = 10.0\n",
    "attr_d_gp_coe=10.0\n",
    "attr_d_lr = 0.0001\n",
    "g_attr_d_coe = 1.0\n",
    "extra_checkpoint_freq = 1000\n",
    "num_packing = 1\n",
    "\n",
    "\n",
    "# config\n",
    "run_config = tf.ConfigProto()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "sess = tf.Session(config=run_config)\n",
    "\n",
    "\n",
    "with sess.as_default() as sess:\n",
    "    assert tf.get_default_session() is sess\n",
    "    gan = DoppelGANger(\n",
    "        sess=sess, \n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        sample_dir=sample_dir,\n",
    "        time_path=time_path,\n",
    "        epoch=epoch,\n",
    "        batch_size=batch_size,\n",
    "        data_feature=data_feature,\n",
    "        data_attribute=data_attribute,\n",
    "        real_attribute_mask=real_attribute_mask,\n",
    "        data_gen_flag=data_gen_flag,\n",
    "        sample_len=sample_len,\n",
    "        data_feature_outputs=data_feature_outputs,\n",
    "        data_attribute_outputs=data_attribute_outputs,\n",
    "        vis_freq=vis_freq,\n",
    "        vis_num_sample=vis_num_sample,\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        attr_discriminator=attr_discriminator,\n",
    "        d_gp_coe=d_gp_coe,\n",
    "        attr_d_gp_coe=attr_d_gp_coe,\n",
    "        g_attr_d_coe=g_attr_d_coe,\n",
    "        d_rounds=d_rounds,\n",
    "        g_rounds=g_rounds,\n",
    "g_lr=g_lr,\n",
    "d_lr=d_lr,\n",
    "attr_d_lr = attr_d_lr,\n",
    "        num_packing=num_packing,\n",
    "        extra_checkpoint_freq=extra_checkpoint_freq)\n",
    "    \n",
    "    #building & training\n",
    "    gan.build()\n",
    "    \n",
    "# load the weights / change the path accordingly\n",
    "    gan.load(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d67e32c6-d01f-4a48-8d3f-0536a93f1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renormalize_per_sample(data_feature, data_attribute, data_feature_outputs,\n",
    "                           data_attribute_outputs, gen_flags,\n",
    "                           num_real_attribute):\n",
    "    attr_dim = 0\n",
    "    for i in range(num_real_attribute):\n",
    "        attr_dim += data_attribute_outputs[i].dim\n",
    "    attr_dim_cp = attr_dim\n",
    "\n",
    "    fea_dim = 0\n",
    "    for output in data_feature_outputs:\n",
    "        if output.type_ == OutputType.CONTINUOUS:\n",
    "            for _ in range(output.dim):\n",
    "                max_plus_min_d_2 = data_attribute[:, attr_dim]\n",
    "                max_minus_min_d_2 = data_attribute[:, attr_dim + 1]\n",
    "                attr_dim += 2\n",
    "\n",
    "                max_ = max_plus_min_d_2 + max_minus_min_d_2\n",
    "                min_ = np.zeros_like(max_plus_min_d_2 - max_minus_min_d_2)\n",
    "\n",
    "                max_ = np.expand_dims(max_, axis=1)\n",
    "                min_ = np.expand_dims(min_, axis=1)\n",
    "\n",
    "                if output.normalization == Normalization.MINUSONE_ONE:\n",
    "                    data_feature[:, :, fea_dim] = \\\n",
    "                        (data_feature[:, :, fea_dim] + 1.0) / 2.0\n",
    "\n",
    "                data_feature[:, :, fea_dim] = \\\n",
    "                    data_feature[:, :, fea_dim] * (max_ - min_) + min_\n",
    "\n",
    "                fea_dim += 1\n",
    "        else:\n",
    "            fea_dim += output.dim\n",
    "\n",
    "    tmp_gen_flags = np.expand_dims(gen_flags, axis=2)\n",
    "    data_feature = data_feature * tmp_gen_flags\n",
    "\n",
    "    data_attribute = data_attribute[:, 0: attr_dim_cp]\n",
    "\n",
    "    return data_feature, data_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7943484-3d44-43d0-8f1b-d264f2da90d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## list_index is just the WWR=0 case\n",
    "\n",
    "def gen_annual (gt_feat_all,given_att_all,list_index,num_gen,location_index,renorm_factor):\n",
    "    \n",
    "    site_list = ['geneva','milan','paris','perlin','zurich']\n",
    "    \n",
    "    num_weeks = 52\n",
    "    \n",
    "    train_sample_size = given_att_all.shape[0]\n",
    "    \n",
    "    index = 3*list_index*num_weeks+int(location_index*train_sample_size/5)\n",
    "    \n",
    "    given_att = given_att_all[index:index+num_weeks,:-2]\n",
    "    given_att_tiles=np.tile(given_att,(num_gen,1))\n",
    "    \n",
    "    length = int(gt_feat_all.shape[1] / sample_len)\n",
    "    num_real_attribute=8\n",
    "    \n",
    "    addi_attribute_input_noise = gan.gen_attribute_input_noise(\n",
    "                num_gen*num_weeks)\n",
    "    feature_input_noise = gan.gen_feature_input_noise(\n",
    "                num_gen*num_weeks, length)\n",
    "    \n",
    "    #print(feature_input_noise)\n",
    "    input_data = gan.gen_feature_input_data_free(\n",
    "                num_gen*num_weeks)\n",
    "    # generate features, attributes and lengths\n",
    "    gen_features, gen_attributes, gen_flags, lengths = gan.sample_from(\n",
    "        None, addi_attribute_input_noise,\n",
    "        feature_input_noise, input_data, given_attribute=given_att_tiles)\n",
    "\n",
    "\n",
    "    #denormalise accordingly\n",
    "    gen_features, gen_attributes = renormalize_per_sample(\n",
    "        gen_features, gen_attributes, data_feature_outputs,\n",
    "        data_attribute_outputs, gen_flags,\n",
    "        num_real_attribute=num_real_attribute)\n",
    "    \n",
    "    \n",
    "    \n",
    "    gt = gt_feat_all[index:index+num_weeks,:]\n",
    "    gen_features=gen_features*renorm_factor\n",
    "    gen_features=gen_features.squeeze(2).reshape(num_gen,-1)\n",
    "    \n",
    "    gen_features = gen_features.transpose()\n",
    "    \n",
    "    gen_features = np.expand_dims(gen_features,0)\n",
    "    gt = gt.reshape(1,-1,1)\n",
    "    \n",
    "    return gt, gen_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccfe5ecb-3b4c-4b7e-94ee-3a0384d187db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_annual_batch (gt_feat_all,given_att_all,num,num_gen,location_index,renorm_factor):\n",
    "    weather_data_array_morning_batch = np.zeros((364,4,num_gen))\n",
    "    weather_data_array_morning_single = np.zeros((364,4,1))\n",
    "    weather_data_array_evening_batch = np.zeros((364,3,num_gen))\n",
    "    weather_data_array_evening_single = np.zeros((364,3,1))\n",
    "    \n",
    "    feat_gt_all = np.empty(shape=(0,8736,1))\n",
    "    feat_gen_all = np.empty(shape=(0,8736,num_gen))\n",
    "    \n",
    "    for i in range(num):\n",
    "        \n",
    "        gt_i, gen_i = gen_annual(gt_feat_all,given_att_all,i,num_gen,4,renorm_factor)\n",
    "        gt_i_daily = gt_i.reshape(364,-1,1)\n",
    "        gen_i_daily = gen_i.reshape(364,-1,num_gen)\n",
    "        \n",
    "        gt_i_daily_complete = np.concatenate((weather_data_array_morning_single,gt_i_daily,weather_data_array_evening_single),axis=1)\n",
    "        gen_i_daily_complete = np.concatenate((weather_data_array_morning_batch,gen_i_daily,weather_data_array_evening_batch),axis=1)\n",
    "        \n",
    "        gt_i_complete = np.expand_dims(gt_i_daily_complete.reshape(-1,1),0)\n",
    "        gen_i_complete = np.expand_dims(gen_i_daily_complete.reshape(-1,num_gen),0)\n",
    "        \n",
    "        feat_gt_all = np.append(feat_gt_all,gt_i_complete).reshape(-1,8736,1)\n",
    "        feat_gen_all = np.append(feat_gen_all,gen_i_complete).reshape(-1,8736,num_gen)\n",
    "        \n",
    "    return feat_gt_all, feat_gen_all\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4ad84ca-a1a5-44e3-92a7-eedc0bf9dccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_gt = np.load('europe_feat_test.npy')\n",
    "#features_gt = np.load('seasia_feat_test.npy')\n",
    "\n",
    "features_gt = features_gt.reshape(-1,119,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8615edf-7716-4b3f-a850-c1906770044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_list = ['geneva','milan','paris','perlin','zurich']\n",
    "#site_list =['hochiminh','jakarta','kualalumpur','pangkok','singapore']\n",
    "for site_i in range(5):\n",
    "    site = site_list[site_i]\n",
    "    site_i_gt, site_i_gen = all_annual_batch(features_gt,data_attribute_normlized,1000,10,site_i,data_attribute_max[48])\n",
    "    site_i_gt_file = site+'_gt_feat_test.npy'\n",
    "    site_i_gen_file = site+'_gen_feat_test.npy' \n",
    "    \n",
    "    np.save(site_i_gt_file,site_i_gt)\n",
    "    np.save(site_i_gen_file,site_i_gen)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python35",
   "language": "python",
   "name": "python35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
